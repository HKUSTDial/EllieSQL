<h1 align="center">ğŸ˜EllieSQL: Cost-Efficient Text-to-SQL <br> with Complexity-Aware Routing</h1>
<h4 align="center">ğŸš§ Please note that this repository is still under construction! ğŸš§</h4>

Official repository for the paper *"EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing"*.

[![Homepage](https://img.shields.io/badge/ğŸ -Homepage-blue)](https://elliesql.github.io/)
[![arXiv](https://img.shields.io/badge/arXiv-2503.22402-<COLOR>.svg)](https://arxiv.org/abs/2503.22402)
[![Checkpoints](https://img.shields.io/badge/ğŸ¤—-Checkpoints-orange)](https://huggingface.co/derrickzhu/EllieSQL_Router_Checkpoints)

![teaser](asserts/teaser.png)

## ğŸ“¢News

[March 25, 2025] ğŸ˜EllieSQL is publicly released!

## ğŸ“–Overview

Text-to-SQL automatically translates natural language queries to SQL, allowing non-technical users to retrieve data from databases without specialized SQL knowledge. Despite the success of advanced LLM-based Text-to-SQL approaches on leaderboards, their unsustainable computational costsâ€”often overlookedâ€”stand as the "elephant in the room" in current leaderboard-driven research, limiting their economic practicability for real-world deployment and widespread adoption. 

To tackle this, we exploratively propose EllieSQL, a complexity-aware routing framework that assigns queries to suitable SQL generation pipelines based on estimated complexity. We investigate multiple routers to direct simple queries to efficient approaches while reserving computationally intensive methods for complex cases. Drawing from economics, we introduce the Token Elasticity of Performance (TEP) metric, capturing cost-efficiency by quantifying the responsiveness of performance gains relative to token investment in SQL generation. Experiments show that compared to always using the most advanced methods in our study, EllieSQL with the Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising performance on Bird development set, achieving more than a 2Ã— boost in TEP over non-routing approaches. This not only advances the pursuit of cost-efficient Text-to-SQL but also invites the community to weigh resource efficiency alongside performance, contributing to progress in sustainable Text-to-SQL.


## ğŸ“‚Project Structure

```
ğŸ˜EllieSQL/
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ config_example.yaml     # A provided config example
â”‚   â”œâ”€â”€ config.yaml             # Config file for LLM API keys and needed paths
â”‚   â””â”€â”€ xxx_config.yaml         # Config for SFT
â”œâ”€â”€ data/                       # Data files and databases
â”œâ”€â”€ examples/                   # Examples for usage
â”œâ”€â”€ scripts/                    # Bash scripts
â”œâ”€â”€ src/                        # Source code for the project
â”‚   â”œâ”€â”€ core/                   # Core functionalities and utilities
â”‚   â”œâ”€â”€ evaluation/             # Evaluation scripts
â”‚   â”œâ”€â”€ modules/                # Modular components for Text-to-SQL
â”‚   â”‚   â”œâ”€â”€ schema_linking/     # Schema linking modules
â”‚   â”‚   â”œâ”€â”€ sql_generation/     # SQL generation modules
â”‚   â”‚   â”œâ”€â”€ post_processing/    # Post-processing modules
â”‚   â”‚   â””â”€â”€ base.py             # Base classes for modules
â”‚   â”œâ”€â”€ router/                 # Implementation of Routers
â”‚   â”œâ”€â”€ sft/                    # Data preparation for SFT and implementation of SFT
â”‚   â”œâ”€â”€ dpo/                    # Data preparation for DPO and implementation of DPO
â”‚   â”œâ”€â”€ pipeline.py             # Pipeline management for processing queries
â”‚   â”œâ”€â”€ pipeline_factory.py     # Factory for creating different pipeline of levels
â”‚   â”œâ”€â”€ run_base.py             # Runner for the base methods: G_B, G_M, G_A
â”‚   â””â”€â”€ run_routing.py          # Runner for the ğŸ˜EllieSQL with routers
â””â”€â”€ README.md                   # This Readme
```

## âš™Environment Setup

1. Create and activate a conda environment:

   ```bash
   conda create --name elliesql python=3.12
   conda activate elliesql
   cd EllieSQL
   pip install -r requirements.txt
   ```

2. Download required resources:

   - Bird dataset: [Bird Official Website](https://bird-bench.github.io/)
   - Router checkpoints: Available on [ğŸ¤—Hugging Face](https://huggingface.co/derrickzhu/EllieSQL_Router_Checkpoints)

3. Pre-process dataset file into the format required for this project:

   ```bash
   python -m data.preprocess
   ```

4. Configure your settings:

   ```bash
   cp config/config_example.yaml config/config.yaml
   ```

   Edit `config/config.yaml` to set up your:

   - API Base Url
   - API keys
   - Dataset paths
   - Model paths
   - Other configurations

   ```yaml
   # Inference with proprietary LLMs via API
   api:
     base_url: "your_base_url"                                # e.g., "https://api.openai.com/v1"
     api_key: "your_api_key"                                  # e.g., "sk-xxxxxxxxxxxxxxxxxxxxxx"
   
   # Inference with local open-source LLMs is also supported
   local_models:
     "qwen2.5-coder-7b-instruct":
       path: "/path/to/Qwen2.5-Coder-7B-Instruct"
       prompt_format: "qwen"
     "xxxxx":                                                 # you can also add new models
       .....
   
   # Set paths and directories
   paths:
     data_dir: "./data"                                       # root for data file
     database_dir: "./data/databases"                         # dir to databases in Bird
     results_dir: "./results"                                 # dir to store results
     logs_dir: "./logs"                                       # dir to store logs
     qwen_dir: "/path/to/Qwen2.5-0.5B"                        # dir to base model to train routers
     roberta_dir: "/path/to/roberta-base"
     sft_data_dir: "./data/sft"                               # dir to prepared data to fine-tune routers
     pairwise_data_dir: "./data/pairwise"
     cascade_data_dir: "./data/cascade"
     dpo_data_dir: "./data/dpo"
     sft_save_dir: "/path/to/saves/Qwen2.5-0.5B-router/sft"   # dir to saved weights after fine-tuning
     roberta_save_dir: "/path/to/saves/RoBERTa-router"
     pairwise_save_dir: "/path/to/saves/Qwen2.5-0.5B-router/pairwise"
     cascade_qwen_save_dir: "/path/to/saves/Qwen2.5-0.5B-router/cascade"
     cascade_roberta_save_dir: "/path/to/saves/RoBERTa-router/cascade"
     dpo_save_dir: "/path/to/saves/Qwen2.5-0.5B-router/dpo"
   ```

## ğŸ’»Hardware Requirements

- Training: 4 Ã— NVIDIA RTX 4090
- Inference: 1 Ã— NVIDIA RTX 4090

## ğŸ§ªRunning Experiments

### Base Methods

1. Ensure your configuration in `config/config.yaml` is correct
2. Run base experiments:

```bash
bash scripts/exp/run_base.sh
```

### Routing Methods

1. Download router checkpoints
2. Update checkpoint paths in `scripts/exp/run_routing.sh`
3. Run routing experiments:

```bash
bash scripts/exp/run_routing.sh
```

Note: Remember to specify the correct GPU device in the scripts according to your setup.

## âš—ï¸Training Routers

We provide scripts for training different types of routers:

- Classification-based routers
- Cascading routers
- Preference learning-based routers

Example: Training a RoBERTa-based classifier router:

```bash
bash scripts/sft/roberta_classifier_sft.sh
```

You can also change hyperparameters in fine-tuning by editing SFT configs in  `config/`.

## âœï¸Citation

If you find our work useful or inspiring, please kindly cite:

```
@misc{zhu2025elliesql,
      title={EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing}, 
      author={Yizhang Zhu and Runzhi Jiang and Boyan Li and Nan Tang and Yuyu Luo},
      year={2025},
      eprint={2503.22402},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2503.22402}, 
}
```
